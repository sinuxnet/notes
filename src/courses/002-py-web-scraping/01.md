# Web Scraping

3 tools to learn:

1. requests
2. Beautiful Soup
3. Scrapy

## Sending http requests

First step of web scraping is sending http requests. We use `requests` package in order to do our first step.

Getting start with `requests`:

```python
import requests

# GET
res = requests.get('https://x.com/home')

# POST
res = requests.post('https://x.com/posts',
                   data = {'key': 'value'})
# PUT
res = requests.put('https://x.com/posts/31',
                   data = {'key': 'value'})
# DELETE
res = requests.delete('https://x.com/posts/31')

# HEAD
res = requests.head('https://x.com/posts')

# OPTIONS
res = requests.options('https://x.com/posts/31')
```

```python
import requests


url = 'https://www.wikipedia.org/'
response = requests.get(url)
data = ""

if response.status_code == 200:
    data = response.text
else:
    data = f"Error: {response.status_code} - {response.reason}"

print(data)

```

## Beautiful Soup

How to install?

```shell
pip install beautifulsoup4
```

```python
import requests
from bs4 import BeautifulSoup

url = "https://en.wikipedia.org/wiki/Mao_Zedong"
response = requests.get("https://en.wikipedia.org/wiki/Mao_Zedong")

content = BeautifulSoup(response.text)

print(content)
```

:warning: The code successfully will execute but we have this warning:

> GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system ("html.parser"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.
>
> The code that caused this warning is on line 6 of the file C:\python\web-scraping\03.py. To get rid of this warning, pass the additional argument 'features="html.parser"' to the BeautifulSoup constructor.

We usually use default `html.parser` but there are another parsers like:

- lxml
- lxml-xml
- html5lib

So to get rid of parser error we define beautifulsoup object like this:

```python
content = BeautifulSoup(response.text, "html.parser")
```

### bs4 methods

#### find()

It finds just first appearance of input value in content.

```python
print(content.find('h1'))
# <h1 class="firstHeading mw-first-heading" id="firstHeading"><span class="mw-page-title-main">Mao Zedong</span></h1>

print(content.find('h2'))
# <h2 class="vector-pinnable-header-label">Contents</h2>

```

#### find_all()

It finds all appearance of input value in content. It returns a list of values.

```python
print(content.find_all('h1'))
# [<h1 class="firstHeading mw-first-heading" id="firstHeading"><span class="mw-page-title-main">Mao Zedong</span></h1>]

print(content.find_all('h2'))
# [<h2 class="vector-pinnable-header-label">Contents</h2>, <h2 id="English_romanisation_of_name">English romanisation of name</h2>, <h2 id="Early_life">Early life</h2>, <h2 id="Early_revolutionary_activity">Early revolutionary activity</h2>]

print(content.find_all(['h1', 'h2']))
# [<h1 class="firstHeading mw-first-heading" id="firstHeading"><span class="mw-page-title-main">Mao Zedong</span></h1>, <h2 class="vector-pinnable-header-label">Contents</h2>, <h2 id="English_romanisation_of_name">English romanisation of name</h2>, <h2 id="Early_life">Early life</h2>, <h2 id="Early_revolutionary_activity">Early revolutionary activity</h2>]
```

How to get just attributes or just value of elements?

```python
print(content.find('h1').attrs)
# {'id': 'firstHeading', 'class': ['firstHeading', 'mw-first-heading']}

print(content.find('h1').text)
# Mao Zedong
```

How to find based on attributes?

```python
print(content.find(attrs={'role': 'note'}))
# <div class="hatnote navigation-not-searchable" role="note">For the TV series, see <a href="/wiki/Mao_Zedong_(TV_series)" title="Mao Zedong (TV series)"><i>Mao Zedong</i> (TV series)</a>.</div>

```

> [!NOTE]
>
> BS4's recommendation to find `class` attribute is use `class_` instead of `class` itself to avoid probable syntax error.

```python
print(content.find(class_='something'))
```

```python
print(content.find('h1', class_='something'))
```

```python
print(content.find('h1').get('id'))
# firstHeading
```

Use find with regex:

```python
import re

print(content.find(re.compile('^d')))
```

#### select()

It runs CSS selector against a parsed document and return all matching elements.

```python
print(content.select('li > a[title="Benjamin Tucker"]'))
# [<a href="/wiki/Benjamin_Tucker" title="Benjamin Tucker">Tucker</a>]
```

### Doing a Real World Example

```python
import requests
from bs4 import BeautifulSoup


url = 'https://en.wikipedia.org/wiki/List_of_Game_of_Thrones_episodes'
response = requests.get(url)

content = BeautifulSoup(response.text, 'html.parser')

episodes = []
ep_tables = content.select("table.wikiepisodetable")


for table in ep_tables:
    headers = []
    rows = table.find_all('tr')

    for header in table.find('tr').find_all('th'):
        headers.append(header.text)

    for row in table.find_all('tr')[1:]:
        values = []
        for col in row.find_all(['th', 'td']):
            values.append(col.text)

        if values:
            # episodes_dict = {headers[i]: values[i] for i in range(len(values))}
            episodes_dict = {}
            for i in range(len(values)):
                episodes_dict[headers[i]] = values[i]

            episodes.append(episodes_dict)

for episode in episodes:
    print(episode)

```
